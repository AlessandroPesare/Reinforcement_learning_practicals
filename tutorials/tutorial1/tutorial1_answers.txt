Question 1:
Think of a (preferably creative) application of reinforcement learning.
Specify the states, actions, and rewards as well as what is needed to satisfy the Markov property.

Reinforcement Learning for Knowledge Graph Reasoning:
Knowledge graphs represent information as entities (nodes) and relationships (edges).
While they're powerful, they often have missing links.
Here's how RL could be applied:
States
The state space consists of:
- Current entity node in the knowledge graph (randomly selected at the first step or others smartest criteria)
- History of traversed entities and relations in the current reasoning path
Query type being answered (e.g., "find the capital of X")
Goal entity or attribute to be discovered

Actions
The agent can:

Follow any outgoing edge from the current entity to another entity
Backtrack to a previously visited node
Terminate the search and return the current entity as the answer
Apply a logical operation to combine information from multiple paths

Rewards

Positive reward when the agent reaches the correct entity that answers the query
Small negative reward for each step taken (encourages efficient paths)
Larger negative reward for returning an incorrect answer
Larger rewards for discovering novel but valid reasoning paths

Markov Property
To satisfy the Markov property (future states depend only on the current state):

The state representation must include the current position in the graph
For multi-hop reasoning, we need to include the history of traversed nodes/relations in the state
The state should encode the query being answered
--------------------------------------------------------------------------------------
Question 2:
Goal-directed learning task that is not an MDP. Try to find a goal-directed learning task that cannot be represented by a Markov decision process.

Language Learning with Context-Dependent Meanings
Language learning, particularly understanding context-dependent meanings and cultural references,
cannot be fully represented as an MDP because:

Understanding a sentence often requires information from far beyond the immediate context.
For example, resolving pronouns or understanding callbacks to earlier conversations may depend on
information from arbitrarily far in the past.
The state (including speaker intentions, cultural context...) is never fully observable to the learner.
Ambiguous State Transitions: The same action (saying a phrase) can lead to different states
depending on cultural context, tone, that are not captured in any finite state representation.
Reward Ambiguity: There's no clear immediate reward signal for using language appropriately in all contexts.

This learning task violates the Markov property because the true state of the environment depends on the entire history of interactions and cultural context, not just the current observable state.
--------------------------------------------------------------------------------------
Question 3:
as â€“ ğœ–-greedy action selection.
Assume that ğœ–-greedy action selection is used.
(a) Suppose |ğ’œ| = 4 and ğœ– = 0.2. When using ğœ–-greedy action selection, what is the probability that the greedy action is selected?
(b) Which value of ğœ– would achieve a probability of 70% of selecting the greedy action?
(c) Generalize the formula for calculating the probability of selecting the greedy action in ğœ–-greedy action selection for any |ğ’œ| and any ğœ–.

a. 0.8 + 0.2 * 1/4 = 0.85
b. 1 - epsilon + 1/4 * epsilon = 0.75 --> epsilon = 0.4
c. 1 - epsilon + 1/|A| * epsilon
--------------------------------------------------------------------------------------
Question 4:
armonic step sizes. Show that the step sizes
ğ›¼_ğ‘›âˆ¶= 1/ğ‘ğ‘›+ğ‘   ğ‘,ğ‘âˆˆR
(where ğ‘ âˆˆ R+ and ğ‘ âˆˆ R are chosen such that ğ‘ğ‘›+ğ‘ â‰  0) satisfy the convergence conditions:
âˆ‘ğ›¼_ğ‘› = âˆ, âˆ‘(ğ›¼_ğ‘›)^2 <âˆ.  ğ‘› = 1...âˆ


âˆ‘ğ›¼_ğ‘› = âˆ‘1/ğ‘ğ‘›+ğ‘ <= âˆ‘1/ğ‘ğ‘› = 1/ğ‘ * âˆ‘1/ğ‘› harmonic series known to diverge
then âˆ‘ğ›¼_ğ‘› = âˆ

âˆ‘(ğ›¼_ğ‘›)^2 = âˆ‘1/(ğ‘ğ‘›+ğ‘)^2 <= âˆ‘1/(ğ‘ğ‘›)2 = 1/ğ‘^2 * âˆ‘1/ğ‘›^2

p-series with exponent p>1 which is known to converge.
--------------------------------------------------------------------------------------
Question 5:
Unbiased step sizes. We use the iteration ğ‘„1 âˆˆ R,
ğ‘„_ğ‘›+1 âˆ¶=ğ‘„_ğ‘› +ğ›¼_ğ‘›(ğ‘…_ğ‘› âˆ’ğ‘„_ğ‘›), ğ‘›â‰¥1.
to estimate ğ‘„_ğ‘› using ğ‘…_ğ‘›, where
ğ›¼_ğ‘› âˆ¶= ğ›¼/ğ›½_ğ‘› , ğ›¼ âˆˆ (0, 1), ğ‘› â‰¥ 1,
and ğ›½_0 âˆ¶= 0, ğ›½_ğ‘› âˆ¶=ğ›½_ğ‘›âˆ’1 +ğ›¼(1âˆ’ğ›½_ğ‘›âˆ’1), ğ‘›â‰¥1,

Show that the iteration for ğ‘„_ğ‘› above yields an exponential
recency-weighted average without initial bias
(i.e., the ğ‘„_ğ‘› do not depend on the initial value ğ‘„_1).

ğ›½_0 âˆ¶= 0, ğ›½_ğ‘› âˆ¶=ğ›½_ğ‘›âˆ’1 +ğ›¼(1âˆ’ğ›½_ğ‘›âˆ’1), ğ‘›â‰¥1,
ğ›¼_ğ‘› âˆ¶= ğ›¼/ğ›½_ğ‘›
ğ‘„_1 is unknown
ğ‘„_ğ‘›+1 âˆ¶=ğ‘„_ğ‘› +ğ›¼_ğ‘›(ğ‘…_ğ‘› âˆ’ğ‘„_ğ‘›), ğ‘›â‰¥1.

n = 1:
ğ›½_1 = ğ›½_0 + ğ›¼(1âˆ’ğ›½_0) = ğ›¼
ğ›¼_1 âˆ¶= ğ›¼/ğ›½_1 = 1
ğ‘„_2 = ğ‘„_1 + ğ›¼_1(ğ‘…_1 âˆ’ğ‘„_1) = ğ‘…_1
This means that after the very first update,
the estimate is exactly equal to the first reward and ğ‘„_1
has been completely replaced by the observed data, ensuring that
the process is unbiased with respect to the initial value.

n>1
ğ‘„_ğ‘›+1 âˆ¶=ğ‘„_ğ‘› +ğ›¼_ğ‘›(ğ‘…_ğ‘› âˆ’ğ‘„_ğ‘›)
ğ›½_n = ğ›½_n-1 + ğ›¼(1âˆ’ğ›½_n-1) = ğ›¼ + ğ›½_n-1(1-ğ›¼)
This recurrence defines a function f such that:
f(Î²)=(1âˆ’ğ›¼)Î²+ğ›¼.
A function f is called a contraction if there exists a constant L
with 0<L<1 such that for every x,y:
âˆ£f(x)âˆ’f(y)âˆ£â‰¤Lâˆ£xâˆ’yâˆ£.
How do we verify this for our function?
f(Î²)=(1âˆ’ğ›¼)Î²+ğ›¼
Calculate the derivative:
The derivative of f with respect to Î² is:
fâ€²(Î²)=1âˆ’ğ›¼.
Check the contraction condition:
Since ğ›¼âˆˆ(0,1) we have:
âˆ£fâ€²(Î²)âˆ£=âˆ£1âˆ’ğ›¼âˆ£<1.
This tells us that f shrinks the distance between any two points
by a factor of at most âˆ£1âˆ’ğ›¼âˆ£ which is less than 1.
Since f is a contraction on a complete space (for instance, the real numbers R)
the theorem guarantees:
Existence and Uniqueness: There exists a unique fixed point
Î²^* such that Î²^*=f(Î²^*)
Convergence: For any initial value Î²_0 the sequence defined by
Î²_n=f(Î²_n-1) converges to Î²^*

A fixed point of a function or recurrence is a value that doesn't
change when the function is applied. In other words, if you have a
function g and Î² is a fixed point then:
Î² = g(Î²)

In our case:

Î²_n = (1-ğ›¼)Î²_n-1 + ğ›¼, a fixed point Î² must satisfy:

Î² = (1âˆ’ğ›¼)Î² + ğ›¼
Î² = 1 --> ğ›¼_ğ‘› âˆ¶= ğ›¼/ğ›½_ğ‘› = ğ›¼
ğ‘„_ğ‘›+1 âˆ¶=ğ‘„_ğ‘›(1-ğ›¼)+ ğ›¼ğ‘…_ğ‘›, ğ‘›â‰¥1.
This is the standard formula for an exponential moving average.
--------------------------------------------------------------------------------------
Question 6:
Multi-armed bandits with ğœ–-greedy action selection (programming).
You play against a 10-armed bandit, where at the beginning of each
episode the true value ğ‘âˆ—(ğ‘), ğ‘ âˆˆ {1, ... , 10}, of each of the 10
actions is chosen to be normally distributed with mean zero and unit
variance. The rewards after choosing action/bandit ğ‘ are normally
distributed with mean ğ‘âˆ—(ğ‘) and unit variance. Using the simple
bandit algorithm and ğœ–-greedy action selection, you have 1000 time
steps or tries in each episode to maximize the average reward starting
from zero knowledge about the bandits. Which value of ğœ– maximizes
the average reward? Which value of ğœ– maximizes the percentage of
optimal actions taken?

see run_experiments.py
-epsilon = 0.1
-epsilon = 0.1

Medium article:
https://medium.com/@ym1942/exploring-multi-armed-bandit-problem-epsilon-greedy-epsilon-decreasing-ucb-and-thompson-02ad0ec272ee
--------------------------------------------------------------------------------------
Question 7:
mab/ucb â€“ Multi-armed bandits with upper-confidence-bound action selection (programming).
This exercise is the same as in Exercise mab/eps, but now the actions
ğ´ğ‘¡ âˆ¶=argmax(ğ‘„ğ‘¡(ğ‘)+ğ‘âˆš lnğ‘¡ ) ğ‘ ğ‘ğ‘¡(ğ‘)
are selected according to the upper-confidence bound. Which value of ğ‘ yields the largest average reward?

see run_experiments.py
- c = 1
--------------------------------------------------------------------------------------
Question 8:
mab/softmax â€“ Multi-armed bandits with soft-max action selection (programming).
This exercise is the same as Exercise mab/eps, but now the actions ğ´ğ‘¡ âˆˆ ğ’œ = {1, ... , |ğ’œ|} are selected with probability
P[ğ‘] = exp(ğ‘„ğ‘¡(ğ‘)/ğœ) , âˆ‘|ğ’œ| exp(ğ‘„ğ‘¡(ğ‘–)/ğœ)
ğ‘–=1
where the parameter ğœ is called the temperature. This probability
distribution is called the soft-max or Boltzmann distribution.
What are the effects of low and high temperatures, i.e., how does the temperature influence the probability distribution all else being equal? Which value of ğœ yields the largest average reward?

Ï„ (Temperature):
Low Ï„
When Ï„ is very small, the exponent exp(Qt(a)/Ï„)	magnifies the differences between action values.
The action with the highest Qt(a) will have almost all the probability, making the selection almost deterministic (greedy).
High Ï„
When Ï„ is very large, the exponent scales down the differences, and the probabilities for all actions tend to become similar,
leading to nearly uniform random selection.
Empirical results suggest that a temperature around Ï„=0.1 typically yields the largest average reward in the 10-armed bandit problem.
see run_experiments.py
--------------------------------------------------------------------------------------
Question 9:
mdp/g1 â€“ Returns and episodes. Suppose ğ›¾âˆ¶=1/2 and the rewards ğ‘…1 âˆ¶=1,ğ‘…2 âˆ¶=âˆ’1,ğ‘…3 âˆ¶=2,
ğ‘…4 âˆ¶= âˆ’1, and ğ‘…5 âˆ¶= 2 are received in an episode with length ğ‘‡ âˆ¶= 5. What are ğº0,...,ğº5?
ğºğ‘¡ âˆ¶= âˆ‘ ğ›¾^[ğ‘˜âˆ’(ğ‘¡+1)] ğ‘…ğ‘˜ =ğ‘…ğ‘¡+1 +ğ›¾ğ‘…ğ‘¡+2 +â‹¯, k = t+1....T
ğºğ‘¡ = ğ‘…ğ‘¡+1 + Gt+1
G0 = 1
G1 = -1 + 1 = 0
G2 = 2 + 1/2*0 = 2
G3 = -1 + 1/2 * 2 = 0
G4 = 2 + 1/2 * 0 = 2
G5 = 0
--------------------------------------------------------------------------------------
Question 10:
Suppose ğ›¾ âˆ¶= 0.9 and the reward sequence starts with ğ‘…1 âˆ¶= âˆ’1 and ğ‘…2 âˆ¶= 2 and is followed
by an infinite sequence of 1s. What are ğº0, ğº1, and ğº2?

G0=R1+Î³R2+Î³^2R3+Î³^3R4....=âˆ’1+0.9â‹…2+Î³^2â‹…1+Î³^3â‹…1+â€¦
=âˆ’1+1.8+âˆ‘0.9^k k>=2
= -1+1.8+(0.9^2/1-0.9) = -1+1.8+8.1 = 8.9

G1=R2+Î³R3+Î³^2R4....=2+0.9â‹…1+Î³^2â‹…1+Î³^3â‹…1+â€¦
= 2.9+âˆ‘0.9^k k>=2 = 2.9 + 8.1 = 11

G2=R3+Î³R4+Î³^2R5 = 1+0.9+âˆ‘0.9^k k>=2 = 1.9 + 8.1 = 10
--------------------------------------------------------------------------------------
Question 11:
Give an equation for ğ‘£ğœ‹ in terms of ğ‘ğœ‹ and ğœ‹.
v_Ï€(s)= âˆ‘ Ï€(aâˆ£s)q_Ï€(s,a)     stocastic case
        aâˆˆA(s)
v_Ï€(s)= q_Ï€(s,Ï€(s))     deterministic case

this is why:
ğ‘£ğœ‹(ğ‘ ) = ğ”¼ğœ‹[ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ] = ğ”¼ğœ‹[ğ‘…ğ‘¡+1 + ğ›¾ğºğ‘¡+1 âˆ£ ğ‘†ğ‘¡ = ğ‘ ] =
= âˆ‘ ğœ‹(ğ‘|ğ‘ )âˆ‘âˆ‘ğ‘(ğ‘ â€²,ğ‘Ÿâˆ£ğ‘ ,ğ‘)(ğ‘Ÿ+ğ›¾ğ”¼ğœ‹[ğºğ‘¡+1 âˆ£ğ‘†ğ‘¡+1 =ğ‘ â€²])
  ğ‘âˆˆğ’œ(ğ‘ )  ğ‘ â€²âˆˆğ’®, ğ‘ŸâˆˆR
ğ‘£ğœ‹(ğ‘ ') = ğ”¼ğœ‹[ğºğ‘¡+1 âˆ£ğ‘†ğ‘¡+1 =ğ‘ â€²]
where
q_Ï€(s,a) = âˆ‘âˆ‘ğ‘(ğ‘ â€²,ğ‘Ÿâˆ£ğ‘ ,ğ‘)(ğ‘Ÿ+ğ›¾ğ”¼ğœ‹[ğºğ‘¡+1 âˆ£ğ‘†ğ‘¡+1=ğ‘ â€²])
           ğ‘ â€²âˆˆğ’®, ğ‘ŸâˆˆR
hence: v_Ï€(s)= âˆ‘ Ï€(aâˆ£s)q_Ï€(s,a)
        aâˆˆA(s)
--------------------------------------------------------------------------------------
Question 12:
mdp/q â€“ Equation for ğ‘ğœ‹.
Give an equation for ğ‘ğœ‹ in terms of ğ‘£ğœ‹ and the four-argument ğ‘.
q_Ï€(s,a) = âˆ‘âˆ‘ğ‘(ğ‘ â€²,ğ‘Ÿâˆ£ğ‘ ,ğ‘)(ğ‘Ÿ+ğ›¾ğ‘£ğœ‹(ğ‘ '))
--------------------------------------------------------------------------------------
Question 13:
mdp/ret â€“ Change of return.
In episodic tasks and in continuing tasks, how does the return ğºğ‘¡ change if a constant ğ‘
is added to all rewards ğ‘…ğ‘¡?
ğºğ‘¡' = ğºğ‘¡ + ğ‘ âˆ‘ ğ›¾^[ğ‘˜âˆ’(ğ‘¡+1)] t = t+1--T
ğºğ‘¡' = ğºğ‘¡ + ğ‘ * 1/1-ğ›¾ when |ğ›¾|<1
--------------------------------------------------------------------------------------
Question 14:
mdp/bellman/qpi â€“ Bellman equation for ğ‘ğœ‹.
Analogous to the derivation of the Bellman equation for ğ‘£ğœ‹, derive the Bellman equation for ğ‘ğœ‹.
ğ‘(ğ‘ ,ğ‘)=ğ”¼ğœ‹[ğºğ‘¡ âˆ£ ğ‘†ğ‘¡ = ğ‘ ,ğ´ğ‘¡ =ğ‘]
= ğ”¼ğœ‹[ğ‘…ğ‘¡+1 + ğ›¾ğºğ‘¡+1 âˆ£ ğ‘†ğ‘¡ = ğ‘ , ğ´ğ‘¡ =ğ‘]
= âˆ‘ ğœ‹(ğ‘|ğ‘ )âˆ‘âˆ‘ğ‘(ğ‘ â€²,ğ‘Ÿâˆ£ğ‘ ,ğ‘)(ğ‘Ÿ+ğ›¾ğ”¼ğœ‹[ğºğ‘¡+1 âˆ£ğ‘†ğ‘¡+1 =ğ‘ â€²,ğ´ğ‘¡ =ğ‘])
= âˆ‘ ğœ‹(ğ‘|ğ‘ )âˆ‘âˆ‘ğ‘(ğ‘ â€²,ğ‘Ÿâˆ£ğ‘ ,ğ‘)(ğ‘Ÿ+ğ›¾ğ‘ğœ‹(ğ‘ â€²))
 aâˆˆA(s) ğ‘ â€²âˆˆğ’®, ğ‘ŸâˆˆR
--------------------------------------------------------------------------------------
Question 15:
 mdp/vstar â€“ Equation for ğ‘£âˆ—.
Give an equation for ğ‘£âˆ— in terms of ğ‘âˆ—.
ğ‘£âˆ—(ğ‘ )= max ğ‘ğœ‹âˆ—(ğ‘ ,ğ‘) ğ‘âˆˆğ’œ(ğ‘ )
--------------------------------------------------------------------------------------
Question 16:
 mdp/qstar â€“ Equation for ğ‘âˆ—.
Give an equation for ğ‘âˆ— in terms of ğ‘£âˆ— and the four-argument ğ‘.
ğ‘âˆ—(ğ‘ ,ğ‘)=ğ”¼[ğ‘…ğ‘¡+1 +ğ›¾ğ‘£âˆ—(ğ‘†ğ‘¡+1)âˆ£ğ‘†ğ‘¡ =ğ‘ , ğ´ğ‘¡ =ğ‘]
âˆ€(ğ‘ ,ğ‘)âˆˆğ’®Ã—ğ’œ(ğ‘ ).
= âˆ‘âˆ‘ğ‘(ğ‘ â€²,ğ‘Ÿâˆ£ğ‘ ,ğ‘)(ğ‘Ÿ+ğ›¾ v_*(ğ‘ â€²))
--------------------------------------------------------------------------------------
Question 17:
mdp/pistar/vstar â€“ Equation for ğœ‹âˆ—. Give an equation for ğœ‹âˆ— in terms of ğ‘âˆ—.
ğœ‹*(ğ‘ ) âˆ¶= arg max ğ‘*ğœ‹(ğ‘ , ğ‘), ğ‘âˆˆğ’œ(ğ‘ )
This means that in each state s the optimal policy chooses any action that achieves the maximum
qâˆ—(s,a)
--------------------------------------------------------------------------------------
Question 18:
mdp/pistar/qstar â€“ Equation for ğœ‹âˆ—.
Give an equation for ğœ‹âˆ— in terms of ğ‘£âˆ— and the four-argument ğ‘.
ğœ‹*(ğ‘ ) âˆ¶= arg max ğ‘*ğœ‹(ğ‘ , ğ‘) = âˆ‘âˆ‘ğ‘(ğ‘ â€²,ğ‘Ÿâˆ£ğ‘ ,ğ‘)(ğ‘Ÿ+ğ›¾ğ‘£âˆ—(ğ‘ â€²)) ğ‘âˆˆğ’œ(ğ‘ ) âˆ€ğ‘ âˆˆğ’®.
                            ğ‘ â€²âˆˆğ’® ğ‘ŸâˆˆR